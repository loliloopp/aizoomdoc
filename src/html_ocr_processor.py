"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ HTML —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ OCR —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
"""

import re
import json
import logging
import html as html_module
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


@dataclass
class HtmlBlock:
    """–ë–ª–æ–∫ –∏–∑ HTML OCR —Ñ–∞–π–ª–∞."""
    block_id: str
    block_number: int  # –ù–æ–º–µ—Ä –±–ª–æ–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    page_number: int
    block_type: str  # text, image, table
    content: str  # HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–ª–∏ —Ç–µ–∫—Å—Ç
    
    # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    crop_url: Optional[str] = None
    zone_name: Optional[str] = None
    content_summary: Optional[str] = None
    detailed_description: Optional[str] = None
    ocr_text: Optional[str] = None
    key_entities: List[str] = field(default_factory=list)


@dataclass
class HtmlOcrDocument:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML OCR —Ñ–∞–π–ª–∞."""
    pdf_path: str
    generated_date: str
    blocks: List[HtmlBlock]
    text_blocks: List[HtmlBlock]
    image_blocks: List[HtmlBlock]
    blocks_by_page: Dict[int, List[HtmlBlock]]  # page ‚Üí blocks


class HtmlOcrProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä HTML —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ OCR."""
    
    # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –±–ª–æ–∫–∞ (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç —Å ID)
    # "–ë–ª–æ–∫ #1 (—Å—Ç—Ä. 2) | –¢–∏–ø: text | ID: 7LPV-EU9..."
    HEADER_PATTERN_OLD = re.compile(
        r'–ë–ª–æ–∫\s+#(\d+)\s+\(—Å—Ç—Ä\.\s+(\d+)\)\s+\|\s+–¢–∏–ø:\s+(\w+)\s+\|\s+ID:\s+([\w-]+)'
    )
    
    # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –±–ª–æ–∫–∞ (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ ID)
    # "–ë–ª–æ–∫ #1 (—Å—Ç—Ä. 1) | –¢–∏–ø: text"
    HEADER_PATTERN_NEW = re.compile(
        r'–ë–ª–æ–∫\s+#(\d+)\s+\(—Å—Ç—Ä\.\s+(\d+)\)\s+\|\s+–¢–∏–ø:\s+(\w+)'
    )
    
    # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ ID –±–ª–æ–∫–∞
    # "BLOCK: 7LPV-EU9J-WJQ"
    BLOCK_ID_PATTERN = re.compile(r'BLOCK:\s+([\w-]+)')
    
    @staticmethod
    def process(html_path: Path) -> Tuple[str, HtmlOcrDocument]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç HTML OCR —Ñ–∞–π–ª.
        
        Args:
            html_path: –ü—É—Ç—å –∫ HTML —Ñ–∞–π–ª—É
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Ç–µ–∫—Å—Ç –¥–ª—è LLM, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç)
        """
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            title_elem = soup.find('h1')
            pdf_path = title_elem.get_text(strip=True) if title_elem else "document.pdf"
            
            gen_date_elem = soup.find('p')
            generated_date = ""
            if gen_date_elem:
                date_text = gen_date_elem.get_text(strip=True)
                if '–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ:' in date_text:
                    generated_date = date_text.replace('–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ:', '').strip()
            
            # –ü–∞—Ä—Å–∏–º –≤—Å–µ –±–ª–æ–∫–∏
            blocks = []
            block_divs = soup.find_all('div', class_='block')
            
            for block_div in block_divs:
                parsed_block = HtmlOcrProcessor._parse_block(block_div)
                if parsed_block:
                    blocks.append(parsed_block)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –±–ª–æ–∫–∏
            text_blocks = [b for b in blocks if b.block_type == 'text']
            image_blocks = [b for b in blocks if b.block_type == 'image']
            
            blocks_by_page = {}
            for block in blocks:
                page = block.page_number
                if page not in blocks_by_page:
                    blocks_by_page[page] = []
                blocks_by_page[page].append(block)
            
            document = HtmlOcrDocument(
                pdf_path=pdf_path,
                generated_date=generated_date,
                blocks=blocks,
                text_blocks=text_blocks,
                image_blocks=image_blocks,
                blocks_by_page=blocks_by_page
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è LLM
            llm_text = HtmlOcrProcessor._format_for_llm(document)
            
            return llm_text, document
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ HTML OCR {html_path}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return f"[–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ HTML: {html_path.name}]\n", None
    
    @staticmethod
    def _parse_block(block_div) -> Optional[HtmlBlock]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω –±–ª–æ–∫ –∏–∑ HTML."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞
            header_div = block_div.find('div', class_='block-header')
            if not header_div:
                return None
            
            header_text = header_div.get_text(strip=True)
            
            # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç (—Å ID –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ)
            match = HtmlOcrProcessor.HEADER_PATTERN_OLD.search(header_text)
            block_id_from_header = None
            if match:
                block_number = int(match.group(1))
                page_number = int(match.group(2))
                block_type = match.group(3)
                block_id_from_header = match.group(4)
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç (–±–µ–∑ ID –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ)
                match = HtmlOcrProcessor.HEADER_PATTERN_NEW.search(header_text)
                if not match:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞: {header_text}")
                    return None
                block_number = int(match.group(1))
                page_number = int(match.group(2))
                block_type = match.group(3)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–ª–æ–∫–∞
            content_div = block_div.find('div', class_='block-content')
            if not content_div:
                return None
            
            # –ò—â–µ–º ID –±–ª–æ–∫–∞ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ (BLOCK: xxx)
            block_id_full = block_id_from_header
            block_id_p = content_div.find('p')
            if block_id_p:
                id_match = HtmlOcrProcessor.BLOCK_ID_PATTERN.search(block_id_p.get_text())
                if id_match:
                    block_id_full = id_match.group(1)
            
            # –ï—Å–ª–∏ ID –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ, –Ω–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            if not block_id_full:
                block_id_full = f"block_{page_number}_{block_number}"
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if block_type == 'image':
                return HtmlOcrProcessor._parse_image_block(
                    content_div, block_id_full, block_number, page_number
                )
            else:  # text, table
                return HtmlOcrProcessor._parse_text_block(
                    content_div, block_id_full, block_number, page_number, block_type
                )
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–ª–æ–∫–∞: {e}")
            return None
    
    @staticmethod
    def _parse_image_block(
        content_div,
        block_id: str,
        block_number: int,
        page_number: int
    ) -> Optional[HtmlBlock]:
        """–ü–∞—Ä—Å–∏—Ç –±–ª–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            pre_elem = content_div.find('pre')
            if not pre_elem:
                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ JSON - —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                logger.info(f"–ë–ª–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {block_id} –±–µ–∑ JSON, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ crop_url")
                
                crop_url = None
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º "–û—Ç–∫—Ä—ã—Ç—å... –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏..."
                link_elem = content_div.find('a', string=re.compile(r'–û—Ç–∫—Ä—ã—Ç—å.*–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏', re.IGNORECASE))
                if link_elem and link_elem.get('href'):
                    crop_url = link_elem['href']
                
                # Fallback: –∏—â–µ–º –ª—é–±—É—é —Å—Å—ã–ª–∫—É –Ω–∞ PDF –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if not crop_url:
                    for a_tag in content_div.find_all('a', href=True):
                        href = a_tag['href']
                        if href.endswith('.pdf') or any(href.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                            crop_url = href
                            break
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                text_content = content_div.get_text(strip=True)
                
                return HtmlBlock(
                    block_id=block_id,
                    block_number=block_number,
                    page_number=page_number,
                    block_type='image',
                    content=text_content,
                    crop_url=crop_url,
                    content_summary=f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}"
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML entities
            json_text = html_module.unescape(pre_elem.get_text())
            
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç Markdown –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞
            json_text = re.sub(r'^```[a-zA-Z]*\s*', '', json_text, flags=re.MULTILINE)
            json_text = re.sub(r'^```\s*', '', json_text, flags=re.MULTILINE)
            json_text = json_text.strip()
            
            data = None
            
            # –ü–∞—Ä—Å–∏–º JSON
            try:
                data = json.loads(json_text)
            except json.JSONDecodeError:
                # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ JSON-–æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ–¥—Ä—è–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Extra data")
                try:
                    results = []
                    decoder = json.JSONDecoder()
                    pos = 0
                    while pos < len(json_text):
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
                        while pos < len(json_text) and json_text[pos].isspace():
                            pos += 1
                        if pos >= len(json_text):
                            break
                        
                        try:
                            obj, idx = decoder.raw_decode(json_text, pos)
                            results.append(obj)
                            pos = idx
                        except json.JSONDecodeError:
                            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å —Ç–µ–∫—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â—É—é —Å–∫–æ–±–∫—É {
                            next_brace = json_text.find('{', pos + 1)
                            if next_brace != -1:
                                pos = next_brace
                            else:
                                break
                    
                    if results:
                        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤, –ø—ã—Ç–∞–µ–º—Å—è –∏—Ö –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å
                        if len(results) == 1:
                            data = results[0]
                        else:
                            # –û–±—ä–µ–¥–∏–Ω—è–µ–º analysis –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
                            merged_analysis = {
                                "content_summary": [],
                                "detailed_description": [],
                                "clean_ocr_text": [],
                                "key_entities": [],
                                "location": {}
                            }
                            
                            has_analysis = False
                            for res in results:
                                an = res.get('analysis', res if 'content_summary' in res else None)
                                if an:
                                    has_analysis = True
                                    if an.get("content_summary"):
                                        merged_analysis["content_summary"].append(an.get("content_summary"))
                                    if an.get("detailed_description"):
                                        merged_analysis["detailed_description"].append(an.get("detailed_description"))
                                    
                                    ocr = an.get("clean_ocr_text") or an.get("ocr_text")
                                    if ocr:
                                        merged_analysis["clean_ocr_text"].append(ocr)
                                        
                                    if an.get("key_entities"):
                                        merged_analysis["key_entities"].extend(an.get("key_entities", []))
                                    
                                    if an.get("location"):
                                        merged_analysis["location"].update(an.get("location"))
                            
                            if has_analysis:
                                data = {
                                    "analysis": {
                                        "content_summary": " ".join(merged_analysis["content_summary"]),
                                        "detailed_description": " ".join(merged_analysis["detailed_description"]),
                                        "clean_ocr_text": " ".join(merged_analysis["clean_ocr_text"]),
                                        "key_entities": list(set(merged_analysis["key_entities"])), # unique
                                        "location": merged_analysis["location"]
                                    }
                                }
                            else:
                                # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã analysis –Ω–µ—Ç, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π –æ–±—ä–µ–∫—Ç
                                data = results[0]

                except Exception as merge_err:
                     logger.warning(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è JSON (merge) –≤ –±–ª–æ–∫–µ {block_id}: {merge_err}")
            
            if data is None:
                # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –≤ –±–ª–æ–∫–µ {block_id}")
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–ª–æ–∫ –±–µ–∑ JSON –¥–∞–Ω–Ω—ã—Ö, –Ω–æ —Å crop_url –µ—Å–ª–∏ –µ—Å—Ç—å
                crop_url = None
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º "–û—Ç–∫—Ä—ã—Ç—å... –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏..."
                link_elem = content_div.find('a', string=re.compile(r'–û—Ç–∫—Ä—ã—Ç—å.*–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏', re.IGNORECASE))
                if link_elem and link_elem.get('href'):
                    crop_url = link_elem['href']
                
                # Fallback: –∏—â–µ–º –ª—é–±—É—é —Å—Å—ã–ª–∫—É –Ω–∞ PDF –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if not crop_url:
                    for a_tag in content_div.find_all('a', href=True):
                        href = a_tag['href']
                        if href.endswith('.pdf') or any(href.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                            crop_url = href
                            break
                
                return HtmlBlock(
                    block_id=block_id,
                    block_number=block_number,
                    page_number=page_number,
                    block_type='image',
                    content=json_text,
                    crop_url=crop_url
                )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å –æ–±–µ—Ä—Ç–∫–æ–π "analysis" –∏–ª–∏ –±–µ–∑)
            if 'analysis' in data:
                analysis = data['analysis']
            else:
                analysis = data
            
            location = analysis.get('location', {})
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º crop_url
            crop_url = None
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º "–û—Ç–∫—Ä—ã—Ç—å... –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏..."
            link_elem = content_div.find('a', string=re.compile(r'–û—Ç–∫—Ä—ã—Ç—å.*–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏', re.IGNORECASE))
            if link_elem and link_elem.get('href'):
                crop_url = link_elem['href']
            
            # Fallback: –∏—â–µ–º –ª—é–±—É—é —Å—Å—ã–ª–∫—É –Ω–∞ PDF –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if not crop_url:
                for a_tag in content_div.find_all('a', href=True):
                    href = a_tag['href']
                    if href.endswith('.pdf') or any(href.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                        crop_url = href
                        break
            
            return HtmlBlock(
                block_id=block_id,
                block_number=block_number,
                page_number=page_number,
                block_type='image',
                content=json_text,
                crop_url=crop_url,
                zone_name=location.get('zone_name'),
                content_summary=analysis.get('content_summary'),
                detailed_description=analysis.get('detailed_description'),
                ocr_text=analysis.get('ocr_text') or analysis.get('clean_ocr_text'),
                key_entities=analysis.get('key_entities', [])
            )
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–ª–æ–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {block_id}: {e}")
            return None
    
    @staticmethod
    def _parse_text_block(
        content_div,
        block_id: str,
        block_number: int,
        page_number: int,
        block_type: str
    ) -> Optional[HtmlBlock]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫."""
        try:
            # –ö–ª–æ–Ω–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            content_copy = content_div.__copy__()
            
            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–π <p>BLOCK: ...</p>
            first_p = content_copy.find('p')
            if first_p and HtmlOcrProcessor.BLOCK_ID_PATTERN.search(first_p.get_text()):
                first_p.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            text_content = content_copy.get_text(separator='\n', strip=True)
            
            return HtmlBlock(
                block_id=block_id,
                block_number=block_number,
                page_number=page_number,
                block_type=block_type,
                content=text_content
            )
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –±–ª–æ–∫–∞ {block_id}: {e}")
            return None
    
    @staticmethod
    def _format_for_llm(document: HtmlOcrDocument) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç HTML –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ç–µ–∫—Å—Ç –¥–ª—è LLM."""
        lines = []
        lines.append(f"[HTML OCR –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø: {document.pdf_path}]\n")
        
        if document.generated_date:
            lines.append(f"–î–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {document.generated_date}\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append(f"–í—Å–µ–≥–æ –±–ª–æ–∫–æ–≤: {len(document.blocks)}")
        lines.append(f"  - –¢–µ–∫—Å—Ç–æ–≤—ã—Ö: {len(document.text_blocks)}")
        lines.append(f"  - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(document.image_blocks)}")
        lines.append(f"–°—Ç—Ä–∞–Ω–∏—Ü: {len(document.blocks_by_page)}\n")
        
        # –ö–∞—Ç–∞–ª–æ–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if document.image_blocks:
            lines.append("## –ö–ê–¢–ê–õ–û–ì –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô\n")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É –∑–æ–Ω—ã
            by_zone = {}
            for block in document.image_blocks:
                zone = block.zone_name or "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"
                if zone not in by_zone:
                    by_zone[zone] = []
                by_zone[zone].append(block)
            
            for zone_name, blocks in sorted(by_zone.items()):
                lines.append(f"### {zone_name} ({len(blocks)})")
                for block in blocks[:15]:  # –ü–µ—Ä–≤—ã–µ 15
                    summary = block.content_summary or "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è"
                    lines.append(
                        f"  - [{block.block_id}] –°—Ç—Ä.{block.page_number}: {summary[:80]}..."
                    )
                    if block.crop_url:
                        lines.append(f"    URL: {block.crop_url}")
                if len(blocks) > 15:
                    lines.append(f"  ... –∏ –µ—â—ë {len(blocks) - 15} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                lines.append("")
        
        # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (–∫—Ä–∞—Ç–∫–æ–µ)
        lines.append("## –°–û–î–ï–†–ñ–ê–ù–ò–ï –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú\n")
        for page_num in sorted(document.blocks_by_page.keys())[:10]:  # –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
            blocks = document.blocks_by_page[page_num]
            text_count = sum(1 for b in blocks if b.block_type == 'text')
            image_count = sum(1 for b in blocks if b.block_type == 'image')
            
            lines.append(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {text_count} —Ç–µ–∫—Å—Ç., {image_count} –∏–∑–æ–±—Ä.")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
            for block in blocks:
                if block.block_type == 'text' and block.content:
                    first_line = block.content.split('\n')[0][:60]
                    if first_line:
                        lines.append(f"  - {first_line}...")
                elif block.block_type == 'image' and block.content_summary:
                    lines.append(f"  - üñºÔ∏è {block.content_summary[:60]}...")
        
        if len(document.blocks_by_page) > 10:
            lines.append(f"... –∏ –µ—â—ë {len(document.blocks_by_page) - 10} —Å—Ç—Ä–∞–Ω–∏—Ü\n")
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
        lines.append("\n## –ò–ù–°–¢–†–£–ö–¶–ò–Ø")
        lines.append("- –î–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π text –±–ª–æ–∫–∏")
        lines.append("- –î–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–π crop_url –∏–∑ image –±–ª–æ–∫–æ–≤")
        lines.append("- ID –±–ª–æ–∫–∞ —Å–≤—è–∑—ã–≤–∞–µ—Ç HTML –∏ JSON —Ñ–∞–π–ª—ã")
        lines.append("- –ò—Å–ø–æ–ª—å–∑—É–π zone_name –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–∏–ø—É\n")
        
        return "\n".join(lines)
    
    @staticmethod
    def search_text(document: HtmlOcrDocument, query: str) -> List[HtmlBlock]:
        """
        –ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –≤ HTML –¥–æ–∫—É–º–µ–Ω—Ç–µ.
        
        Args:
            document: HTML –¥–æ–∫—É–º–µ–Ω—Ç
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        """
        query_lower = query.lower()
        results = []
        
        for block in document.text_blocks:
            if query_lower in block.content.lower():
                results.append(block)
        
        # –ü–æ–∏—Å–∫ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö (ocr_text, key_entities)
        for block in document.image_blocks:
            if block.ocr_text and query_lower in block.ocr_text.lower():
                results.append(block)
            elif block.content_summary and query_lower in block.content_summary.lower():
                results.append(block)
            elif block.key_entities:
                for entity in block.key_entities:
                    if query_lower in entity.lower():
                        results.append(block)
                        break
        
        return results

